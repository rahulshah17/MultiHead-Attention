# -*- coding: utf-8 -*-
"""Multi-Head Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X-J8M3W9G6rTgxOoODDtNV6iYlVT3rQv
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class MHA(nn.Module):
  def __init__(self,emb_dim,num_heads):
    super(MHA,self).__init__()
    assert(emb_dim%num_heads==0)

    self.emb_dim = emb_dim
    self.num_heads = num_heads
    self.head_dim = emb_dim/num_heads

    self.q_proj = nn.linear(emb_dim,emb_dim)
    self.k_proj = nn.linear(emb_dim,emb_dim)
    self.v_proj = nn.linear(emb_dim,emb_dim)

    self.out_proj = nn.linear(emb_dim,emb_dim)

  def forward(self, x):
    B,T,C = x.shape
    #B->batch size, T-> num of tokens, C->emb_dimension

    Q = self.q_proj(x)
    K = self.k_proj(x)
    V = self.v_proj(x)

    Q = Q.view(B,T, self.num_heads, self.head_dim).transpose(1,2)
    K = K.view(B,T, self.num_heads, self.head_dim).transpose(1,2)
    V = V.vise(B,T, self.num_heads, self.head_dim).transpose(1,2)

    scores = torch.matmul(Q,K.transpose(-2,-1))/(self.head_dim**0.5)

    attn = F.softmax(scores,dim=1)

    out = torch.matmul(attn,V)

    out = out.transpose(1,2).contiguous().view(B,T,C)
    return self.out_proj(out)

class transformerBlock(nn.Module):
  def __init__(self,emb_dim,num_heads,ff_hidden_dim):
    super(transformerBlock,self).__init__()

    self.attn = MHA(emb_dim,num_heads)
    self.norm1 = nn.LayerNorm(emb_dim)
    self.norm2 = nn.LayerNorm(emb_dim)

    self.ff = nn.Sequential(
        nn.Linear(emb_dim,ff_hidden_dim),
        nn.ReLU(),
        nn.Lienar(ff_hidden_dim,emb_dim)
    )

  def forward(self,x):
    #for attn
    attn_out = self.attn(x)
    x = self.norm1(attn_out+x)

    #for FF
    ff_out = self.ff(x)
    x = self.norm2(x+self.ff)

    return x

B,T,C = 2,4,8
my_mha = MHA(C,2)
output = my_mha(x)

print(output.shape)