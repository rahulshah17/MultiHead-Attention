# MultiHead-Attention
## My Implimentation of the Multi Head Attention Mechanism
- Projecting the input embeddings into queries (Q), keys (K), and values (V).
- Splitting them into multiple heads.
- Calculating scaled dot-product attention in each head.
- Concatenating all head outputs.
- Applying a final linear layer to get the output.
